{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fork元: https://github.com/richarddwang/electra_pytorch\n",
    "\n",
    "miyamonzがこれを読解した上で、コードの整理とコメントの追加などをしたものです。\n",
    "\n",
    "以下に依存をしています。\n",
    "- pytorch, fastai\n",
    "- ELECTRAのモデル定義に関しては、huggingface/transformers\n",
    "- 学習データはhuggingface/datasets\n",
    "\n",
    "モデルのレイヤーの定義をメジャーなライブラリに頼ることで、全体のコード量が減っていて把握しやすいかと思います。\n",
    "\n",
    "行数が多く読解が難しいのは以下あたりかとおもいます\n",
    "\n",
    "- 前処理の部分（ELECTRADataProcessor）\n",
    "- 事前学習タスクの定義部分（ELECTRAModel, ELECTRALoss）\n",
    "\n",
    "\n",
    "元のPretrain.ipynbからの変更ポイント\n",
    "\n",
    "- star importをやめた\n",
    "- _utilsのフォルダのうち、pretrianでしか使われていないコードをpretrain/_utilsに移動\n",
    "- ノートブック内で記述された長い処理を、別ファイルに切り出してpretrain/*.pyに移動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM, ElectraForPreTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuraton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もともとは_utilsにあったが、シンプルなクラスなのでnotebook直書きに移動\n",
    "\n",
    "ただ単に`c[\"hoge\"]`を`c.hoge`で書けるようにするだけのクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = MyConfig({\n",
    "    'device': 'cuda:0',\n",
    "    'base_run_name': 'vanilla',  # run_name = {base_run_name}_{seed}\n",
    "    'seed': 11081,  # 11081 36 1188 76 1 4 4649 7 # None/False to randomly choose seed from [0,999999]\n",
    "\n",
    "    'adam_bias_correction': False,\n",
    "    'schedule': 'original_linear',\n",
    "    'sampling': 'fp32_gumbel',\n",
    "    'electra_mask_style': True,\n",
    "    'gen_smooth_label': False,\n",
    "    'disc_smooth_label': False,\n",
    "\n",
    "    'size': 'small',\n",
    "#     'datas': ['openwebtext'],\n",
    "    'datas': ['wikipedia'],\n",
    "    'logger': None, #\"wandb\",\n",
    "    'num_workers': 3,\n",
    "})\n",
    "\n",
    "\n",
    "\"\"\" Vanilla ELECTRA settings\n",
    "'adam_bias_correction': False,\n",
    "'schedule': 'original_linear',\n",
    "'sampling': 'fp32_gumbel',\n",
    "'electra_mask_style': True,\n",
    "'gen_smooth_label': False,\n",
    "'disc_smooth_label': False,\n",
    "'size': 'small',\n",
    "'datas': ['openwebtext'],\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルでは設定の確認や、設定値から別の設定を定めたい処理などを行っている\n",
    "\n",
    "ほとんど元と同じだが、不要なフォルダ作成処理などは消した"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check and Default\n",
    "assert c.sampling in ['fp32_gumbel', 'fp16_gumbel', 'multinomial']\n",
    "assert c.schedule in ['original_linear', 'separate_linear', 'one_cycle', 'adjusted_one_cycle']\n",
    "for data in c.datas:\n",
    "    assert data in ['wikipedia', 'bookcorpus', 'openwebtext']\n",
    "assert c.logger in ['wandb', 'neptune', None, False]\n",
    "\n",
    "if not c.base_run_name:\n",
    "    c.base_run_name = str(datetime.now(timezone(timedelta(hours=+8))))[6:-13].replace(' ','').replace(':','').replace('-','')\n",
    "if not c.seed:\n",
    "    c.seed = random.randint(0, 999999)\n",
    "\n",
    "c.run_name = f'{c.base_run_name}_{c.seed}'\n",
    "\n",
    "if c.gen_smooth_label is True:\n",
    "    c.gen_smooth_label = 0.1\n",
    "if c.disc_smooth_label is True:\n",
    "    c.disc_smooth_label = 0.1\n",
    "\n",
    "# Setting of different sizes\n",
    "i = ['small', 'base', 'large'].index(c.size)\n",
    "c.mask_prob = [0.15, 0.15, 0.25][i]\n",
    "c.lr = [5e-4, 2e-4, 2e-4][i]\n",
    "c.bs = [128, 256, 2048][i]\n",
    "c.steps = [10**6, 766*1000, 400*1000][i]\n",
    "c.max_length = [128, 512, 512][i]\n",
    "generator_size_divisor = [4, 3, 4][i]\n",
    "\n",
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-discriminator')\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-generator')\n",
    "# note that public electra-small model is actually small++ and don't scale down generator size \n",
    "gen_config.hidden_size = int(disc_config.hidden_size/generator_size_divisor)\n",
    "gen_config.num_attention_heads = disc_config.num_attention_heads//generator_size_divisor\n",
    "gen_config.intermediate_size = disc_config.intermediate_size//generator_size_divisor\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{c.size}-generator\")\n",
    "\n",
    "# Print info\n",
    "print(f\"process id: {os.getpid()}\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "Path('./checkpoints/pretrain').mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習に必要なテキストデータの用意\n",
    "\n",
    "huggingaface/datasetsを利用している\n",
    "\n",
    "元のノートブックではopenwebtextなどもダウンロードするようになっていたが、私は今後日本語での事前学習をしたいので消した\n",
    "\n",
    "ここは後に別ファイルに切り出す可能性が高い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "def download_dataset():\n",
    "    # download english\n",
    "    # wiki = datasets.load_dataset('wikipedia', '20200501.en')['train']\n",
    "    \n",
    "    # download japanese\n",
    "    wiki = datasets.load_dataset(\n",
    "        './pretrain/wikipedia.py',\n",
    "        beam_runner='DirectRunner',\n",
    "        language='ja',\n",
    "        date='20210120')['train']\n",
    "    return wiki\n",
    "\n",
    "wiki = download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下ではpreprocessを行っている\n",
    "\n",
    "具体的な処理は、ELECTRADataProcessorというクラスで定義されている\n",
    "\n",
    "これは、元のgoogleの実装と同じことをするようにpytorchで書き直されたもの\n",
    "\n",
    "ただし、このクラスは、実装者であるricharddwang氏が書いたhugdatafastというライブラリに依存していた\n",
    "\n",
    "huggingface/datasetsのデータセットとfastaiをよしなにつなぐためのものらしいが、内部で何をしてるのか分かりにくいので、このライブラリへの依存を外した\n",
    "\n",
    "\n",
    "今回のユースケースに置いて、hugdatafastのコードがやっていることは殆どなかったからだ。  \n",
    "（このことを確認してもらうためには、実際にコードを読んで確認してもらうしか無い。とても面倒だった\n",
    "\n",
    "結果として、そのライブラリが持っていたHF_Dataset, MySortedDLというクラスを、不要なものを削除してepretrain/_utils/{hf_dataset.py, mysorteddl.py}にコピーした。\n",
    "\n",
    "この書き換えが、ケアレスミス等が無くうまく言っているか保証がないしちょっと不安なのだが、事前学習はちゃんと動いたので多分大丈夫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from pretrain._utils.electra_dataprocessor import ELECTRADataProcessor\n",
    "data_dir = Path('./data')\n",
    "def preprocess(sources, c, hf_tokenizer, num_proc):\n",
    "    dsets = []\n",
    "    ELECTRAProcessor = partial(\n",
    "        ELECTRADataProcessor, hf_tokenizer=hf_tokenizer, max_length=c.max_length)\n",
    "    \n",
    "    for name, ds in sources.items():\n",
    "        cache_dir = data_dir / \"preprocess\" / f\"{name}_{len(ds)}_{c.max_length}\"\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        path = cache_dir / f\"electra.arrow\"\n",
    "        \n",
    "        cache_file_name = str(path.resolve())\n",
    "        mapped = ELECTRAProcessor(ds).map(cache_file_name=cache_file_name, num_proc=num_proc)\n",
    "        dsets.append(mapped)\n",
    "\n",
    "    assert len(dsets) == len(sources)\n",
    "\n",
    "    train_dset = datasets.concatenate_datasets(dsets)\n",
    "    return train_dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結構時間がかかるので注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# it took about 25min. less num_proc will increase time.\n",
    "# after cache is created, it takes about 40s\n",
    "sources = {\n",
    "    'wiki': wiki,\n",
    "}\n",
    "train_dset = preprocess(sources, c, hf_tokenizer, num_proc=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここらへんは元はricharddwang/hugdatafastのコードがやっていたものなのだが、解体した結果こうなった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain.get_dataloaders import get_dataloader\n",
    "dl = get_dataloader(c, hf_tokenizer, train_dset)\n",
    "\n",
    "from fastai.text.all import DataLoaders\n",
    "dls = DataLoaders(dl, path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dls.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Masked language model objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLM objective callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルにデータを渡す前に、マスクをかけたりするところ  \n",
    "つまりELECTRAの事前学習タスクについての知識ないと難しいかも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain.masked_lm_cb import MaskedLMCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlm_cb = MaskedLMCallback(mask_tok_id=hf_tokenizer.mask_token_id,\n",
    "                          special_tok_ids=hf_tokenizer.all_special_ids,\n",
    "                          vocab_size=hf_tokenizer.vocab_size,\n",
    "                          mlm_probability=c.mask_prob,\n",
    "                          replace_prob=0.0 if c.electra_mask_style else 0.1,\n",
    "                          original_prob=0.15 if c.electra_mask_style else 0.1,\n",
    "                          for_electra=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ELECTRA (replaced token detection objective)\n",
    "see details in paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain.models import ELECTRAModel, ELECTRALoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed & PyTorch benchmark\n",
    "torch.backends.cudnn.benchmark = True\n",
    "dls[0].rng = random.Random(c.seed) # for fastai dataloader\n",
    "random.seed(c.seed)\n",
    "np.random.seed(c.seed)\n",
    "torch.manual_seed(c.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelの用意をしている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator\n",
    "generator = ElectraForMaskedLM(gen_config)\n",
    "discriminator = ElectraForPreTraining(disc_config)\n",
    "discriminator.electra.embeddings = generator.electra.embeddings\n",
    "generator.generator_lm_head.weight = generator.electra.embeddings.word_embeddings.weight\n",
    "\n",
    "# ELECTRA training loop\n",
    "electra_model = ELECTRAModel(generator, discriminator, hf_tokenizer, sampling=c.sampling)\n",
    "electra_loss_func = ELECTRALoss(gen_label_smooth=c.gen_smooth_label, disc_label_smooth=c.disc_smooth_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizerとschedulerは別のファイルに分けた\n",
    "\n",
    "中身については参考元のREADMEのAdvanced detailsをよく読むとよい\n",
    "\n",
    "TODO: optimizerとschedulerについてもう少し詳しく書く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain.optim import get_optim\n",
    "opt_func = get_optim(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pretrain.scheduler import get_scheduler\n",
    "lr_shedule = get_scheduler(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下から、学習の実行になる\n",
    "\n",
    "fastaiのLearnerとか、ここからfastaiで学習する際の呼び出し方の話になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "callbackとして、以下を渡している\n",
    "- MaskedLMCallback\n",
    "- RunSteps\n",
    "\n",
    "RunStepsにて、学習全体の完了の判定と、checkpointの保存などを行っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import Learner\n",
    "from pretrain._utils.run_steps import RunSteps\n",
    "# Learner\n",
    "dls.to(torch.device(c.device))\n",
    "learn = Learner(dls, electra_model,\n",
    "                loss_func=electra_loss_func,\n",
    "                opt_func=opt_func,\n",
    "                path='./checkpoints',\n",
    "                model_dir='pretrain',\n",
    "                cbs=[mlm_cb,\n",
    "                    RunSteps(c.steps, [0.0625, 0.125, 0.25, 0.5, 1.0], c.run_name+\"_{percent}\"),\n",
    "                    ],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if c.logger == 'wandb':\n",
    "  import wandb\n",
    "  from fastai.callback.wandb import  WandbCallback\n",
    "  wandb.init(name=c.run_name, project='electra_pretrain', config={**c})\n",
    "  learn.add_cb(WandbCallback(log_preds=False, log_model=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils.gradient_clipping import GradientClipping\n",
    "# Mixed precison and Gradient clip\n",
    "learn.to_fp16(init_scale=2.**11)\n",
    "learn.add_cb(GradientClipping(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習全体の停止は、既に述べた通りRunStepsというコールバックで制御される。\n",
    "\n",
    "なのでfitにわたすepoch数は適当にでかい数字を入れている\n",
    "\n",
    "\n",
    "なんでこうなっているかは分からないが、RunStepsで制御してる内容が、恐らく素のfastaiのままだと実現できなかったからだろうと思われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining will stop by c.steps and it's controlled by RunSteps callback.\n",
    "# 9999 is no meaning.\n",
    "learn.fit(9999, cbs=[lr_shedule])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
